# This config file documents all available configurations for training, evaluating or enjoying (watching an agent play in realtime) a model!
# These are the defaults that are used if an incomplete config file was provided via the --config argument used.

### ENVIRONMENT CONFIG ###
environment:
  # Environment Type (Unity, ObstacleTower, Minigrid, Procgen, CartPole)
  type: "Unity"
  # type: "Unity"
  # Environment Name (Unity environments have to specify the path to the executable)
  name: "/home/jannik/Documents/Projektgruppe/Executables/Walker_13_07/Walker_13_07"
  # name: "./UnityBuilds/ObstacleTowerReduced/ObstacleTower"
  frame_skip: 1
  last_action_to_obs: False
  last_reward_to_obs: False
  obs_stacks: 1
  grayscale: False
  resize_vis_obs: [84, 84]
  tanh_squashing: True
  reset_params:
    start-seed: 0
    num-seeds: 100
    view-size: 3

### MODEL CONFIG ###
model:
  load_model: False
  model_path: "path/to/model.pt"
  checkpoint_interval: 50
  activation: "tanh"
  vis_encoder: "cnn"
  vec_encoder: "linear" # "linear", "none"
  num_vec_encoder_units: 512
  hidden_layer: "default"
  num_hidden_layers: 3
  num_hidden_units: 512
  num_hidden_pre_head: 512
  tanh_squashing: True
  normalize_observations: False

### EVALUATION CONFIG ###
evaluation:
  evaluate: False
  n_workers: 3
  seeds: [1001, 1002, 1003, 1004, 1005]
  interval: 50

### SAMPLER CONFIG
sampler:
  type: "TrajectorySampler"
  n_workers: 8
  worker_steps: 256

### TRAINER CONFIG ###
trainer:

  algorithm: "PPO".
  resume_at: 0
  gamma: 0.995
  lamda: 0.95
  updates: 10000
  epochs: 3
  refresh_buffer_epoch: -1
  n_mini_batches: 4
  value_coefficient: 0.5
  max_grad_norm: 0.5
  share_parameters: True
  learning_rate_schedule:
    initial: 3.0e-4
    final: 0
    power: 1.0
    max_decay_steps: 10000
  beta_schedule:
    initial: 0.005
    final: 0.005
    power: 1.0
    max_decay_steps: 800
  clip_range_schedule:
    initial: 0.2
    final: 0.2
    power: 1.0
    max_decay_steps: 1000
